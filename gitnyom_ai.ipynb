{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8x1TROIaPgnvSzHEoKnCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aqil18/gitnyom/blob/aqil-ai/gitnyom_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTy-t9NJXqGN",
        "outputId": "ebbbfa6d-fa6f-4fa7-fb59-5927a34c625c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline\n",
        "1. Preprocesses the text\n",
        "2. Applies the model\n",
        "3. Post processes the result\n"
      ],
      "metadata": {
        "id": "FudSha6kX6im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual text data file\n",
        "text_file = \"example_repo.txt\"\n",
        "\n",
        "try:\n",
        "    with open(text_file, 'r', encoding='utf-8') as file:\n",
        "        text_data = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{text_file}' not found. Please upload your text data file.\")\n",
        "    # You might want to add more robust error handling or file upload functionality here\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "QdkDg9DQeFxA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a suitable pre-trained model for causal language modeling\n",
        "model_name = \"gpt2\"  # Example: gpt2, distilgpt2\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH-YCPu3eHiD",
        "outputId": "1856651a-1a7c-4516-e915-446283cd9b88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n",
        "- Breaks down the text into smaller manageable pieces of text"
      ],
      "metadata": {
        "id": "RAbtqfTLeeKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenized_text = tokenizer.encode(text_data, return_tensors=\"tf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3hgsGhUeP7R",
        "outputId": "2889c0cc-4a01-4f08-d27d-9fbf41fae3c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3802 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training hyperparameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)"
      ],
      "metadata": {
        "id": "nFUFIc72eRfS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data with truncation and padding\n",
        "tokenized_text = tokenizer(\n",
        "    text_data,\n",
        "    return_tensors=\"tf\",\n",
        "    truncation=True,  # Truncate sequences longer than the model's maximum input length\n",
        "    padding=\"max_length\",  # Pad shorter sequences to the model's maximum input length\n",
        "    max_length=model.config.max_position_embeddings  # Use the model's maximum input length\n",
        ")\n",
        "\n",
        "# Access the input_ids from the tokenized output\n",
        "input_ids = tokenized_text[\"input_ids\"]\n",
        "\n",
        "# Define training hyperparameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Train the model (adjust epochs and batch_size as needed)\n",
        "model.fit(input_ids, input_ids, epochs=3, batch_size=4)  # Example training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "crxJrk1-ggdw",
        "outputId": "5c443199-0f52-4189-e3af-2f3c63fb2bed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-20be308a980d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize the text data with truncation and padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tokenized_text = tokenizer(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Truncate sequences longer than the model's maximum input length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2859\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2860\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2968\u001b[0m             )\n\u001b[1;32m   2969\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2971\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3037\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3038\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2760\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2762\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2763\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (adjust epochs and batch_size as needed)\n",
        "model.fit(tokenized_text, tokenized_text, epochs=3, batch_size=4) # Example training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1aY7G90AeTNr",
        "outputId": "1969aed8-d4b8-48eb-ed38-da13c5439f36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node tfgpt2lm_head_model_1/transformer/wpe/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-11-2cbb41ebed01>\", line 2, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1229, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 922, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 490, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/core/embedding.py\", line 273, in call\n\nindices[0,1428] = 1428 is not in [0, 1024)\n\t [[{{node tfgpt2lm_head_model_1/transformer/wpe/embedding_lookup}}]] [Op:__inference_train_function_57522]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2cbb41ebed01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model (adjust epochs and batch_size as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Example training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node tfgpt2lm_head_model_1/transformer/wpe/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-11-2cbb41ebed01>\", line 2, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1229, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 1672, in train_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 922, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\", line 890, in run_call_with_unpacked_inputs\n\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 490, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/base_layer.py\", line 1136, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/layers/core/embedding.py\", line 273, in call\n\nindices[0,1428] = 1428 is not in [0, 1024)\n\t [[{{node tfgpt2lm_head_model_1/transformer/wpe/embedding_lookup}}]] [Op:__inference_train_function_57522]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_chatbot\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_chatbot\")\n",
        "\n",
        "print(\"Model trained and saved successfully!\")\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "# Load the fine-tuned model\n",
        "# from transformers import pipeline\n",
        "\n",
        "# model_path = \"fine_tuned_chatbot\"\n",
        "# chatbot = pipeline(\"text-generation\", model=model_path, tokenizer=model_path)\n",
        "\n",
        "# prompt = \"Your prompt here\"\n",
        "# response = chatbot(prompt, max_length=50, num_return_sequences=1)\n",
        "# print(response[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "xzQkrUCVa4mF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}